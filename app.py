from flask import Flask, render_template, request
import requests
from bs4 import BeautifulSoup
from together import Together
import os # Added for env var

app = Flask(__name__)

# Replace with your actual Together API key
api_key = os.environ.get("TOGETHER_API_KEY", "358e7841fdb21acd7b53284312270a2f52496937026e4ac6bafc7bb326b9fbf3")

client = Together(api_key=api_key)

def get_duckduckgo_results(query):
    """Fetch search results from DuckDuckGo."""
    url = f"https://duckduckgo.com/html/?q={query}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    results = soup.find_all('a', class_='result__a')
    search_results = []
    for result in results:
        title = result.get_text()
        link = result.get('href')
        search_results.append({'title': title, 'url': link})
    
    return search_results

def process_query_with_llm(query, search_results):
    """Use the LLM API to generate a contextual answer based on search results."""
    # Generate the context from the search results
    search_context = "\n".join([f"Title: {result['title']} - URL: {result['url']}" for result in search_results])

    # Create a prompt with the search results for the LLM
    prompt = f"User query: {query}\n\nSearch Results:\n{search_context}\n\nAnswer the user's question based on the search results."

    # Call the LLM to process the query with the search results as context
    response = client.chat.completions.create(
        model="meta-llama/Llama-Vision-Free",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=512,
        temperature=0.7,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
        stop=["<|eot_id|>","<|eom_id|>"],
        stream=False
    )

    # Corrected way to access the content in the response
    llm_response = ""
    try:
        # Corrected access to the 'content' inside 'choices[0].message'
        if response.choices and len(response.choices) > 0:
            llm_response = response.choices[0].message.content  # Accessing the content directly from the message
        else:
            print("No valid choices in the LLM response.")
    except Exception as e:
        print(f"Error while processing LLM response: {e}")

    # Collect the sources (URLs)
    sources = "\n".join([f"Source {i+1}: {result['url']}" for i, result in enumerate(search_results)])

    return llm_response, sources
   

@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        user_query = request.form["query"]
        search_results = get_duckduckgo_results(user_query)

        if not search_results:
            return render_template("index.html", error="No results found.")

        llm_response, sources = process_query_with_llm(user_query, search_results)

        if llm_response:
             return render_template("index.html", query=user_query, llm_response=llm_response, sources=sources)
        else:
           return render_template("index.html", query=user_query, error="No answer generated by LLM.")

    return render_template("index.html")


if __name__ == "__main__":
    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000))) # Changed to be publicly accessible
