import streamlit as st
import requests
from bs4 import BeautifulSoup
from together import Together

# Initialize Together API client with the API key
# Use streamlit secrets to manage API Keys
def get_api_key():
    #Get api key from secrets.toml
    return st.secrets["together_api_key"]

# Initialize Together API client with the API key
client = Together(api_key=get_api_key())

def get_duckduckgo_results(query):
    """Fetch search results from DuckDuckGo."""
    url = f"https://duckduckgo.com/html/?q={query}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    results = soup.find_all('a', class_='result__a')
    search_results = []
    for result in results:
        title = result.get_text()
        link = result.get('href')
        search_results.append({'title': title, 'url': link})
    
    return search_results

def process_query_with_llm(query, search_results):
    """Use the LLM API to generate a contextual answer based on search results."""
    # Generate the context from the search results
    search_context = "\n".join([f"Title: {result['title']} - URL: {result['url']}" for result in search_results])

    # Create a prompt with the search results for the LLM
    prompt = f"User query: {query}\n\nSearch Results:\n{search_context}\n\nAnswer the user's question based on the search results."

    # Call the LLM to process the query with the search results as context
    response = client.chat.completions.create(
        model="meta-llama/Llama-Vision-Free",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=512,
        temperature=0.7,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
        stop=["<|eot_id|>","<|eom_id|>"],
        stream=False
    )

    # Print the raw response to understand its structure
    print("LLM Raw Response:", response)

    # Corrected way to access the content in the response
    llm_response = ""
    try:
        # Corrected access to the 'content' inside 'choices[0].message'
        if response.choices and len(response.choices) > 0:
            llm_response = response.choices[0].message.content  # Accessing the content directly from the message
        else:
            st.error("No valid choices in the LLM response.")
    except Exception as e:
        st.error(f"Error while processing LLM response: {e}")

    # Collect the sources (URLs)
    sources = "\n".join([f"Source {i+1}: {result['url']}" for i, result in enumerate(search_results)])

    return llm_response, sources

# Streamlit App
def main():
    st.title("AI Web Search")

    # Get user query
    user_query = st.text_input("What would you like to search for?")

    # Search only when query is entered
    if user_query:
        # Fetch search results from DuckDuckGo
        with st.spinner(f"Fetching DuckDuckGo search results for: {user_query}..."):
            search_results = get_duckduckgo_results(user_query)

        # If there are no results, let the user know
        if not search_results:
            st.error("No results found.")
            return

        # Generate the answer using the LLM, feeding in the search results
        with st.spinner("Getting response from LLM..."):
            llm_response, sources = process_query_with_llm(user_query, search_results)

        if llm_response:
            st.subheader("LLM's Answer:")
            st.write(llm_response)
            st.subheader("Sources Used:")
            st.write(sources)
        else:
             st.error("No answer generated by LLM.")


if __name__ == "__main__":
    main()
